from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
import time
import re
from datetime import datetime
import logging
import threading
import concurrent.futures
import asyncio
from typing import List, Dict, Any, Optional
from schemas import WS_EVENTS, URLItem, PaperBase, URLItemStatus
from utils.socket_manager import socket_manager

logger = logging.getLogger(__name__)

class GoogleScholarSearchTask:
    """
    ğŸ“‹ å•ä¸ªGoogle Scholaræœç´¢ä»»åŠ¡
    æ¯ä¸ªä»»åŠ¡æ‹¥æœ‰ç‹¬ç«‹çš„æµè§ˆå™¨å®ä¾‹ï¼Œæ”¯æŒå¹¶å‘æ‰§è¡Œ
    """
    
    def __init__(self, google_scholar_url: str, client_id: str, search_id: str, headless: bool = False):
        """
        åˆå§‹åŒ–æœç´¢ä»»åŠ¡
        
        Args:
            google_scholar_url (str): Google Scholarä½œè€…é¡µé¢URL
            client_id (str): å®¢æˆ·ç«¯ID
            search_id (str): æœç´¢ID
            headless (bool): æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼
        """
        self.google_scholar_url = google_scholar_url
        self.client_id = client_id
        self.search_id = search_id
        self.headless = headless
        
        # ğŸ—ï¸ æµè§ˆå™¨é…ç½®
        self.options = Options()
        if headless:
            self.options.add_argument("--headless")
        self.options.add_argument("--disable-gpu")
        self.options.add_argument("--window-size=1200,800")
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        self.options.add_argument("--disable-blink-features=AutomationControlled")
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = None
        self.wait = None
        
        # ğŸ“Š æœç´¢çŠ¶æ€
        self.search_data = {
            "client_id": client_id,
            "search_id": search_id,
            "url": google_scholar_url,
            "author_name": "",
            "status": URLItemStatus.PENDING,
            "progress": 0,
            "fetched_paper_count": None,
            "total_paper_count": None,
            "papers_urls": [],
            "papers": [],
            "error_message": "",
            "start_time": datetime.now().isoformat(),
            "thread_id": threading.current_thread().ident
        }
        
        self.search_data = URLItem(**self.search_data)
    
    def _initialize_driver(self):
        """ğŸš€ åˆå§‹åŒ–ç‹¬ç«‹çš„WebDriverå®ä¾‹"""
        try:
            self.driver = webdriver.Chrome(options=self.options)
            self.wait = WebDriverWait(self.driver, 10)
            logger.info(f"ğŸš€ çº¿ç¨‹ {threading.current_thread().ident} WebDriverå·²åˆå§‹åŒ–")
            return True
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ {threading.current_thread().ident} WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = f"WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}"
            return False
    
    async def run(self) -> Dict[str, Any]:
        """
        ğŸ¯ æ‰§è¡Œå®Œæ•´çš„æœç´¢ä»»åŠ¡
        
        Returns:
            Dict[str, Any]: æœç´¢ç»“æœ
        """
        try:
            logger.info(f"ğŸ¯ çº¿ç¨‹ {threading.current_thread().ident} å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡: {self.search_id}")
            
            # ğŸš€ åˆå§‹åŒ–æµè§ˆå™¨
            if not self._initialize_driver():
                await socket_manager.send(WS_EVENTS["FAILED_FETCH_A_GOOGLE_SCHOLAR_URL"], self._serialize_search_data(), self.client_id)
                return self.search_data
            
            # ğŸ“‹ æ”¶é›†Scholarä¿¡æ¯
            if await self._collect_scholar_info():
                await socket_manager.send(WS_EVENTS["UPDATE_FETCH_A_GOOGLE_SCHOLAR_URL_PROCESS"], self._serialize_search_data(), self.client_id)
                # ğŸ“š æœç´¢è®ºæ–‡è¯¦ç»†ä¿¡æ¯
                await self._search_papers_details()
                await socket_manager.send(WS_EVENTS["FETCHED_COMPLETED_WITH_PAPERS_INFO"], self._serialize_search_data(), self.client_id)
            else:
                await socket_manager.send(WS_EVENTS["FAILED_FETCH_A_GOOGLE_SCHOLAR_URL"], self._serialize_search_data(), self.client_id)
            
            logger.info(f"âœ… çº¿ç¨‹ {threading.current_thread().ident} æœç´¢ä»»åŠ¡å®Œæˆ: {self.search_id}")
            
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ {threading.current_thread().ident} æœç´¢ä»»åŠ¡å¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = str(e)
        
        finally:
            # ğŸ”’ æ¸…ç†èµ„æº
            self._cleanup()
        
        return self.search_data
    
    def run_sync(self) -> Dict[str, Any]:
        """
        ğŸ”„ åŒæ­¥åŒ…è£…å™¨ï¼Œç”¨äºåœ¨ThreadPoolExecutorä¸­è¿è¡Œasyncæ–¹æ³•
        
        Returns:
            Dict[str, Any]: æœç´¢ç»“æœ
        """
        try:
            # ğŸ”„ åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œasyncæ–¹æ³•
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self.run())
            finally:
                loop.close()
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ {threading.current_thread().ident} åŒæ­¥åŒ…è£…å™¨æ‰§è¡Œå¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = str(e)
            return self.search_data
    
    async def _collect_scholar_info(self) -> bool:
        """
        ğŸ“‹ æ”¶é›†Google Scholarä½œè€…ä¿¡æ¯å’Œè®ºæ–‡åˆ—è¡¨
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.search_data.status = URLItemStatus.COLLECTING_INFO
            logger.info(f"ğŸ“‹ çº¿ç¨‹ {threading.current_thread().ident} å¼€å§‹æ”¶é›†Scholarä¿¡æ¯...")
            
            # ğŸŒ è®¿é—®URL
            logger.info(f"ğŸŒ æ­£åœ¨è®¿é—®: {self.google_scholar_url}")
            self.driver.get(self.google_scholar_url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # ğŸ‘¤ è·å–ä½œè€…ä¿¡æ¯
            if not await self._extract_author_info():
                return False
            
            # ğŸ“… æŒ‰å¹´ä»½æ’åºè®ºæ–‡
            self._sort_papers_by_year()
            
            # ğŸ“œ åŠ è½½æ‰€æœ‰è®ºæ–‡
            self._load_all_papers()
            
            # ğŸ“Š æ”¶é›†è®ºæ–‡URLs
            if not self._collect_paper_urls():
                return False
            
            self.search_data.fetched_paper_count = 0
            self.search_data.status = URLItemStatus.COLLECTED_INFO
            self.search_data.progress = 25
            logger.info(f"âœ… çº¿ç¨‹ {threading.current_thread().ident} Scholarä¿¡æ¯æ”¶é›†å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ {threading.current_thread().ident} æ”¶é›†Scholarä¿¡æ¯å¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = f"æ”¶é›†Scholarä¿¡æ¯å¤±è´¥: {str(e)}"
            return False
    
    async def _extract_author_info(self) -> bool:
        """ğŸ‘¤ æå–ä½œè€…ä¿¡æ¯"""
        try:
            author_selectors = [
                "#gsc_prf_in",
                ".gsc_prf_in", 
                "h1",
                ".gs_ai_name"
            ]
            
            author_name = ""
            for selector in author_selectors:
                try:
                    author_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    author_name = author_element.text.strip()
                    if author_name:
                        logger.info(f"âœ… ä¿¡æ¯åŒºåŸŸ: {selector}")
                        break
                except NoSuchElementException:
                    continue
            
            if not author_name:
                page_title = self.driver.title
                if " - " in page_title:
                    logger.info(f"âœ… é¡µé¢æ ‡é¢˜: {page_title} åˆ†å‰²å: {page_title.split(' - ')[0].strip()}")
                    author_name = page_title.split(" - ")[0].strip()
            
            if author_name:
                self.search_data.author_name = author_name
                logger.info(f"âœ… ä½œè€…ä¿¡æ¯: {author_name}")
                await socket_manager.send(WS_EVENTS["UPDATE_FETCH_A_GOOGLE_SCHOLAR_URL_PROCESS"], self._serialize_search_data(), self.client_id)
                return True
            else:
                logger.error("âŒ æ— æ³•è·å–ä½œè€…ä¿¡æ¯")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æå–ä½œè€…ä¿¡æ¯å¤±è´¥: {str(e)}")
            return False
    
    def _sort_papers_by_year(self):
        """ğŸ“… æŒ‰å¹´ä»½æ’åºè®ºæ–‡"""
        try:
            logger.info("ğŸ“… æ­£åœ¨æŒ‰å¹´ä»½æ’åºè®ºæ–‡...")
            
            sort_selectors = [
                "#gsc_a_ha",
                "button[aria-label*='Sort']",
                ".gsc_a_ha"
            ]
            
            sort_button = None
            for selector in sort_selectors:
                try:
                    sort_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if sort_button and sort_button.is_displayed():
                        logger.info(f"âœ… æ’åºæŒ‰é’®: {selector}")
                        break
                except NoSuchElementException:
                    continue
            
            if sort_button:
                self.driver.execute_script("arguments[0].click();", sort_button)
                time.sleep(1)
                
                year_sort_options = [
                    "//button[contains(text(), 'Year')]",
                    "//a[contains(text(), 'Year')]",
                    "//option[contains(text(), 'Year')]"
                ]
                
                for xpath in year_sort_options:
                    try:
                        year_option = self.driver.find_element(By.XPATH, xpath)
                        logger.info(f"âœ… å¹´ä»½æ’åºé€‰é¡¹: {xpath}")
                        self.driver.execute_script("arguments[0].click();", year_option)
                        time.sleep(1)
                        break
                    except NoSuchElementException:
                        continue
            
            logger.info("âœ… è®ºæ–‡å·²æŒ‰å¹´ä»½æ’åº")
            
        except Exception as e:
            logger.warning(f"âš ï¸ æŒ‰å¹´ä»½æ’åºå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ’åº: {str(e)}")
    
    def _load_all_papers(self):
        """ğŸ“œ åŠ è½½æ‰€æœ‰è®ºæ–‡ï¼ˆç‚¹å‡»Show moreï¼‰"""
        try:
            logger.info("ğŸ“œ æ­£åœ¨åŠ è½½æ‰€æœ‰è®ºæ–‡...")
            show_more_attempts = 0
            max_attempts = 1000
            
            initial_papers = self.driver.find_elements(By.CSS_SELECTOR, ".gsc_a_tr")
            logger.info(f"ğŸ“‹ åˆå§‹è®ºæ–‡æ•°é‡: {len(initial_papers)}")
            
            while show_more_attempts < max_attempts:
                try:
                    show_more_selectors = [
                        "#gsc_bpf_more",
                        "button[onclick*='more']",
                        ".gsc_bpf_more",
                        "//button[contains(text(), 'Show more')]",
                        "//button[contains(text(), 'SHOW MORE')]",
                        "//a[contains(text(), 'Show more')]"
                    ]
                    
                    show_more_button = None
                    for selector in show_more_selectors:
                        try:
                            if selector.startswith("//"):
                                show_more_button = self.driver.find_element(By.XPATH, selector)
                            else:
                                show_more_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                            
                            if show_more_button and show_more_button.is_displayed() and show_more_button.is_enabled():
                                break
                        except NoSuchElementException:
                            continue
                    
                    if show_more_button and show_more_button.is_displayed() and show_more_button.is_enabled():
                        self.driver.execute_script("arguments[0].scrollIntoView(true);", show_more_button)
                        time.sleep(1)
                        self.driver.execute_script("arguments[0].click();", show_more_button)
                        logger.info(f"ğŸ”„ ç‚¹å‡»Show moreæŒ‰é’® (ç¬¬{show_more_attempts + 1}æ¬¡)")
                        time.sleep(2)
                        show_more_attempts += 1
                    else:
                        logger.info("âœ… æ²¡æœ‰æ›´å¤šè®ºæ–‡å¯åŠ è½½")
                        break
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ç‚¹å‡»Show moreæŒ‰é’®å¤±è´¥: {str(e)}")
                    break
            
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½è®ºæ–‡å¤±è´¥: {str(e)}")
    
    def _collect_paper_urls(self) -> bool:
        """ğŸ“Š æ”¶é›†æ‰€æœ‰è®ºæ–‡URL"""
        try:
            logger.info("ğŸ“Š æ­£åœ¨æ”¶é›†è®ºæ–‡ä¿¡æ¯...")
            
            paper_selectors = [
                ".gsc_a_tr",
                "tr.gsc_a_tr",
                ".gs_r.gs_or.gs_scl",
                ".gsc_a_t"
            ]
            
            papers_urls = []
            paper_elements = []
            
            for selector in paper_selectors:
                try:
                    paper_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if paper_elements:
                        break
                except NoSuchElementException:
                    continue
            
            if paper_elements:
                for paper_element in paper_elements:
                    try:
                        link_selectors = [
                            "a.gsc_a_at",
                            "a",
                            ".gsc_a_at"
                        ]
                        
                        paper_url = None
                        for link_selector in link_selectors:
                            try:
                                link_element = paper_element.find_element(By.CSS_SELECTOR, link_selector)
                                paper_url = link_element.get_attribute("href")
                                if paper_url and paper_url.startswith("http"):
                                    break
                            except NoSuchElementException:
                                continue
                        
                        if paper_url and paper_url not in papers_urls:
                            papers_urls.append(paper_url)
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ æå–è®ºæ–‡URLå¤±è´¥: {str(e)}")
                        continue
            
            total_paper_count = len(papers_urls)
            self.search_data.total_paper_count = total_paper_count
            self.search_data.papers_urls = papers_urls
            
            logger.info(f"âœ… æˆåŠŸæ”¶é›†åˆ° {total_paper_count} ç¯‡è®ºæ–‡")
            logger.info(f"ğŸ“‹ è®ºæ–‡URLæ ·ä¾‹: {papers_urls[:3] if papers_urls else 'æ— '}")
            
            return total_paper_count > 0
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†è®ºæ–‡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return False
    
    async def _search_papers_details(self):
        """ğŸ“š æœç´¢è®ºæ–‡è¯¦ç»†ä¿¡æ¯"""
        try:
            papers_urls = self.search_data.papers_urls
            if not papers_urls:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡URL")
                return
            
            logger.info(f"ğŸ“š å¼€å§‹è·å– {len(papers_urls)} ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯...")
            self.search_data.status = URLItemStatus.SEARCHING_PAPERS
            
            papers = []
            
            for index, paper_url in enumerate(papers_urls):
                self.search_data.fetched_paper_count = index + 1
                self.search_data.progress = 25 + round((index + 1)*1.0 / len(papers_urls) * 70, 2)
                try:
                    logger.info(f"ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {index + 1}/{len(papers_urls)} ç¯‡è®ºæ–‡...")
                    
                    # ğŸŒ è®¿é—®è®ºæ–‡é¡µé¢
                    self.driver.get(paper_url)
                    time.sleep(2)
                    
                    # ğŸ“‹ æå–è®ºæ–‡ä¿¡æ¯
                    paper_info = self._extract_paper_details(paper_url)
                    
                    if paper_info:
                        papers.append(paper_info)
                        logger.info(f"âœ… æˆåŠŸæå–è®ºæ–‡: {paper_info.title[:50]}...")
                    else:
                        logger.warning(f"âš ï¸ æ— æ³•æå–è®ºæ–‡ä¿¡æ¯: {paper_url}")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†è®ºæ–‡å¤±è´¥ {paper_url}: {str(e)}")
                    continue
                await socket_manager.send(WS_EVENTS["UPDATE_FETCH_A_GOOGLE_SCHOLAR_URL_PROCESS"], self._serialize_search_data(), self.client_id)
            
            # ğŸ“Š æ›´æ–°æœ€ç»ˆç»“æœ
            self.search_data.papers = papers
            self.search_data.status = URLItemStatus.COMPLETED
            self.search_data.progress = 100
            
            logger.info(f"âœ… è®ºæ–‡è¯¦ç»†ä¿¡æ¯æœç´¢å®Œæˆï¼ŒæˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢è®ºæ–‡è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = f"æœç´¢è®ºæ–‡è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}"
    
    def _serialize_search_data(self) -> dict:
        """
        ğŸ”„ åºåˆ—åŒ–æœç´¢æ•°æ®ï¼Œå¤„ç†datetimeç­‰ä¸å¯JSONåºåˆ—åŒ–çš„å¯¹è±¡
        
        Returns:
            dict: å¯JSONåºåˆ—åŒ–çš„æœç´¢æ•°æ®
        """
        try:
            # ä½¿ç”¨model_dumpå¹¶æŒ‡å®šåºåˆ—åŒ–æ¨¡å¼
            return self.search_data.model_dump(mode='json')
        except Exception as e:
            logger.warning(f"âš ï¸ åºåˆ—åŒ–æœç´¢æ•°æ®å¤±è´¥: {str(e)}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨å¤„ç†
            data = self.search_data.model_dump()
            
            # å¤„ç†datetimeå­—æ®µ
            if 'start_time' in data and data['start_time']:
                if isinstance(data['start_time'], str):
                    pass  # å·²ç»æ˜¯å­—ç¬¦ä¸²
                else:
                    data['start_time'] = data['start_time'].isoformat() if data['start_time'] else None
            
            # å¤„ç†papersä¸­çš„datetimeå­—æ®µ
            if 'papers' in data and data['papers']:
                for paper in data['papers']:
                    if 'date' in paper and paper['date']:
                        if not isinstance(paper['date'], str):
                            paper['date'] = paper['date'].isoformat() if paper['date'] else ""
            
            return data
    
    def _extract_paper_details(self, paper_url: str) -> Optional[PaperBase]:
        """ğŸ“„ æå–å•ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            paper_info = {
                "title": "",
                "authors": [],
                "year": 0,  # é»˜è®¤å€¼ä¸º0ï¼Œç¨åä¼šæ›´æ–°
                "date": "",
                "url": paper_url,
                "pdf_url": None,
                "citations": 0,  # é»˜è®¤å€¼ä¸º0ï¼Œç¨åä¼šæ›´æ–°
                "publisher": None,
                "paper_type": None,
                "description": None
            }
            
            # ğŸ“‘ æå–æ ‡é¢˜
            title_selectors = [
                ".gs_rt h3 a",
                ".gs_rt a",
                "h1",
                ".citation_title",
                "title"
            ]
            
            for selector in title_selectors:
                try:
                    title_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    title = title_element.text.strip()
                    if title and len(title) > 5:
                        paper_info["title"] = title
                        break
                except NoSuchElementException:
                    continue
            
            # ğŸ‘¥ æå–ä½œè€…ä¿¡æ¯
            authors_selectors = [
                ".gs_a",
                ".citation_author",
                ".authors",
                ".author"
            ]
            
            for selector in authors_selectors:
                try:
                    authors_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    authors_text = authors_element.text.strip()
                    if authors_text:
                        authors = self._parse_authors(authors_text)
                        if authors:
                            paper_info["authors"] = authors
                            break
                except NoSuchElementException:
                    continue
            
            # ğŸ“… æå–å¹´ä»½å’Œæ—¥æœŸ
            year_selectors = [
                ".gs_a",
                ".citation_date",
                ".year",
                ".date"
            ]
            
            for selector in year_selectors:
                try:
                    date_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    date_text = date_element.text.strip()
                    year, date = self._parse_date_info(date_text)
                    if year:
                        paper_info["year"] = year
                        paper_info["date"] = date
                        break
                except NoSuchElementException:
                    continue
            
            # ğŸ”— æå–PDFæˆ–è®ºæ–‡åº“URL
            pdf_selectors = [
                "a[href*='.pdf']",
                ".gs_or_ggsm a",
                ".citation_pdf_url",
                "a[href*='doi.org']",
                "a[href*='arxiv.org']"
            ]
            
            for selector in pdf_selectors:
                try:
                    pdf_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for pdf_element in pdf_elements:
                        href = pdf_element.get_attribute("href")
                        if href and (href.endswith('.pdf') or 'doi.org' in href or 'arxiv.org' in href):
                            paper_info["pdf_url"] = href
                            break
                    if paper_info["pdf_url"]:
                        break
                except NoSuchElementException:
                    continue
            
            # ğŸ“Š æå–å¼•ç”¨æ¬¡æ•°
            citation_selectors = [
                ".gs_fl a[href*='cites']",
                ".citation_count",
                "a[href*='cited']"
            ]
            
            for selector in citation_selectors:
                try:
                    citation_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    citation_text = citation_element.text.strip()
                    citations = self._parse_citations(citation_text)
                    if citations is not None:
                        paper_info["citations"] = citations
                        break
                except NoSuchElementException:
                    continue
            
            # ğŸ“° æå–å‘è¡¨ä¿¡æ¯å’Œç±»å‹
            publisher_selectors = [
                ".gs_a",
                ".citation_venue",
                ".journal",
                ".conference"
            ]
            
            for selector in publisher_selectors:
                try:
                    publisher_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    publisher_text = publisher_element.text.strip()
                    publisher, paper_type = self._parse_publisher_info(publisher_text)
                    if publisher:
                        paper_info["publisher"] = publisher
                        paper_info["paper_type"] = paper_type
                        break
                except NoSuchElementException:
                    continue
            
            # ğŸ“ æå–æè¿°ä¿¡æ¯
            description_selectors = [
                ".gs_rs",
                ".citation_abstract",
                ".abstract",
                ".description"
            ]
            
            for selector in description_selectors:
                try:
                    desc_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    description = desc_element.text.strip()
                    if description and len(description) > 20:
                        paper_info["description"] = description[:500]
                        break
                except NoSuchElementException:
                    continue
            
            if not paper_info["title"]:
                return None
            
            # ğŸ”§ åˆ›å»ºPaperBaseå¯¹è±¡
            try:
                # ç¡®ä¿å¿…éœ€å­—æ®µæœ‰æœ‰æ•ˆå€¼
                if not paper_info["date"]:
                    paper_info["date"] = f"{paper_info['year']}-01-01" if paper_info["year"] > 0 else "1900-01-01"
                
                paper_base = PaperBase(**paper_info)
                return paper_base
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºPaperBaseå¯¹è±¡å¤±è´¥: {str(e)}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ æå–è®ºæ–‡è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def _parse_authors(self, authors_text: str) -> List[str]:
        """ğŸ‘¥ è§£æä½œè€…ä¿¡æ¯"""
        try:
            if " - " in authors_text:
                authors_text = authors_text.split(" - ")[0]
            
            authors = [author.strip() for author in authors_text.split(",")]
            
            cleaned_authors = []
            for author in authors:
                author = re.sub(r'\d{4}', '', author).strip()
                author = re.sub(r'[^\w\s\-\.]', '', author).strip()
                if author and len(author) > 1:
                    cleaned_authors.append(author)
            
            return cleaned_authors
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§£æä½œè€…ä¿¡æ¯å¤±è´¥: {str(e)}")
            return []
    
    def _parse_date_info(self, date_text: str) -> tuple:
        """ğŸ“… è§£ææ—¥æœŸä¿¡æ¯"""
        try:
            year_match = re.search(r'\b(19|20)\d{2}\b', date_text)
            year = int(year_match.group()) if year_match else None
            
            date_patterns = [
                r'\b\d{4}-\d{2}-\d{2}\b',
                r'\b\d{1,2}/\d{1,2}/\d{4}\b',
                r'\b\d{1,2}-\d{1,2}-\d{4}\b'
            ]
            
            date_str = ""
            for pattern in date_patterns:
                date_match = re.search(pattern, date_text)
                if date_match:
                    date_str = date_match.group()
                    break
            
            if not date_str and year:
                logger.warning(f"âš ï¸ è§£ææ—¥æœŸä¿¡æ¯å¤±è´¥: {date_text}")
                date_str = f"{year}-01-01"
            
            return year, date_str
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§£ææ—¥æœŸä¿¡æ¯å¤±è´¥: {str(e)}")
            return -1, ""
    
    def _parse_citations(self, citation_text: str) -> int:
        """ğŸ“Š è§£æå¼•ç”¨æ¬¡æ•°"""
        try:
            patterns = [
                r'Cited by (\d+)',
                r'å¼•ç”¨ (\d+)',
                r'(\d+) citations',
                r'(\d+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, citation_text, re.IGNORECASE)
                if match:
                    return int(match.group(1))
            
            return 0
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§£æå¼•ç”¨æ¬¡æ•°å¤±è´¥: {str(e)}")
            return 0
    
    def _parse_publisher_info(self, publisher_text: str) -> tuple:
        """ğŸ“° è§£æå‘è¡¨å•†å’Œè®ºæ–‡ç±»å‹ä¿¡æ¯"""
        try:
            publisher = publisher_text
            paper_type = ""
            
            bracket_pattern = r'\([^)]*\)'
            publisher_clean = re.sub(bracket_pattern, '', publisher_text).strip()
            
            if publisher_clean:
                publisher = publisher_clean
            
            text_lower = publisher_text.lower()
            
            if any(keyword in text_lower for keyword in ['journal', 'nature', 'science', 'ieee', 'acm']):
                paper_type = "Journal"
            elif any(keyword in text_lower for keyword in ['conference', 'proceedings', 'workshop', 'symposium']):
                paper_type = "Conference"
            elif any(keyword in text_lower for keyword in ['arxiv', 'preprint', 'biorxiv']):
                paper_type = "Preprint"
            else:
                paper_type = "Unknown"
            
            return publisher, paper_type
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§£æå‘è¡¨å•†ä¿¡æ¯å¤±è´¥: {str(e)}")
            return "", ""
    
    def _cleanup(self):
        """ğŸ”’ æ¸…ç†èµ„æº"""
        try:
            if self.driver:
                self.driver.quit()
                self.driver = None
                self.wait = None
                logger.info(f"ğŸ”’ çº¿ç¨‹ {threading.current_thread().ident} æµè§ˆå™¨å·²å…³é—­")
        except Exception as e:
            logger.warning(f"âš ï¸ çº¿ç¨‹ {threading.current_thread().ident} å…³é—­æµè§ˆå™¨å¤±è´¥: {str(e)}")


class GoogleScholarCrawler:
    """
    ğŸ” Google Scholar è®ºæ–‡çˆ¬è™«ç®¡ç†å™¨
    æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘æœç´¢ï¼Œæ¯ä¸ªä»»åŠ¡ä½¿ç”¨ç‹¬ç«‹çš„æµè§ˆå™¨å®ä¾‹
    """
    
    def __init__(self, max_workers: int = 3, headless: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«ç®¡ç†å™¨
        
        Args:
            max_workers (int): æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            headless (bool): æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼
        """
        self.max_workers = max_workers
        self.headless = headless
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        self.running_tasks = {}  # å­˜å‚¨æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        self.completed_tasks = {}  # å­˜å‚¨å·²å®Œæˆçš„ä»»åŠ¡
        self.completed_tasks_order = []  # ğŸ“‹ è®°å½•å®Œæˆä»»åŠ¡çš„é¡ºåº
        self.max_completed_tasks = 20  # ğŸ”¢ æœ€å¤§ä¿å­˜çš„å®Œæˆä»»åŠ¡æ•°é‡
        self._task_lock = threading.Lock()
        
        logger.info(f"ğŸš€ Google Scholar çˆ¬è™«ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
    
    def scholar_info(self, google_scholar_url: str, client_id: str, search_id: str) -> str:
        """
        ğŸ¯ æäº¤æœç´¢ä»»åŠ¡ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
        
        Args:
            google_scholar_url (str): Google Scholarä½œè€…é¡µé¢URL
            client_id (str): å®¢æˆ·ç«¯ID
            search_id (str): æœç´¢ID
        Returns:
            str: ä»»åŠ¡ID
        """
        task_id = f"{client_id}_{search_id}"
        
        with self._task_lock:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒä»»åŠ¡åœ¨è¿è¡Œ
            if task_id in self.running_tasks:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} å·²åœ¨è¿è¡Œä¸­")
                return task_id
            
            # åˆ›å»ºæœç´¢ä»»åŠ¡
            search_task = GoogleScholarSearchTask(
                google_scholar_url=google_scholar_url,
                client_id=client_id,
                search_id=search_id,
                headless=self.headless
            )
            
            # æäº¤åˆ°çº¿ç¨‹æ± ï¼ˆä½¿ç”¨åŒæ­¥åŒ…è£…å™¨ï¼‰
            future = self.executor.submit(search_task.run_sync)
            
            # æ·»åŠ å›è°ƒå‡½æ•°å¤„ç†å®Œæˆçš„ä»»åŠ¡
            future.add_done_callback(lambda f: self._on_task_complete(task_id, f))
            
            self.running_tasks[task_id] = {
                "future": future,
                "task": search_task,
                "start_time": datetime.now()
            }
            
            logger.info(f"ğŸ¯ ä»»åŠ¡ {task_id} å·²æäº¤åˆ°çº¿ç¨‹æ± ")
            return task_id
    
    def _on_task_complete(self, task_id: str, future: concurrent.futures.Future):
        """ğŸ“Š ä»»åŠ¡å®Œæˆå›è°ƒ"""
        with self._task_lock:
            if task_id in self.running_tasks:
                task_info = self.running_tasks.pop(task_id)
                
                try:
                    result = future.result()
                    self.completed_tasks[task_id] = {
                        "result": result,
                        "completed_time": datetime.now(),
                        "start_time": task_info["start_time"]
                    }
                    logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²å®Œæˆ")
                except Exception as e:
                    logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {str(e)}")
                    self.completed_tasks[task_id] = {
                        "result": {"status": "error", "error_message": str(e)},
                        "completed_time": datetime.now(),
                        "start_time": task_info["start_time"]
                    }
                
                # ğŸ“‹ ç®¡ç†å®Œæˆä»»åŠ¡åˆ—è¡¨ï¼Œä¿æŒæœ€å¤š20ä¸ªè®°å½•
                self._manage_completed_tasks(task_id)
    
    def _manage_completed_tasks(self, new_task_id: str):
        """
        ğŸ“‹ ç®¡ç†å®Œæˆä»»åŠ¡åˆ—è¡¨ï¼Œä¿æŒæœ€å¤š20ä¸ªè®°å½•ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
        
        Args:
            new_task_id (str): æ–°å®Œæˆçš„ä»»åŠ¡ID
        """
        # æ·»åŠ æ–°ä»»åŠ¡åˆ°é¡ºåºåˆ—è¡¨
        if new_task_id not in self.completed_tasks_order:
            self.completed_tasks_order.append(new_task_id)
        
        # ğŸ”¢ å¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼Œåˆ é™¤æœ€æ—©çš„ä»»åŠ¡
        while len(self.completed_tasks_order) > self.max_completed_tasks:
            oldest_task_id = self.completed_tasks_order.pop(0)  # ç§»é™¤æœ€æ—©çš„ä»»åŠ¡ID
            
            if oldest_task_id in self.completed_tasks:
                del self.completed_tasks[oldest_task_id]  # åˆ é™¤å¯¹åº”çš„ä»»åŠ¡æ•°æ®
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤æœ€æ—©çš„å®Œæˆä»»åŠ¡è®°å½•: {oldest_task_id}")
        
        logger.info(f"ğŸ“Š å½“å‰ä¿å­˜ {len(self.completed_tasks_order)} ä¸ªå®Œæˆä»»åŠ¡è®°å½•")
    
    def get_search_status(self, client_id: str, search_id: str) -> Dict[str, Any]:
        """
        ğŸ“Š è·å–æœç´¢çŠ¶æ€ä¿¡æ¯
        
        Args:
            client_id (str): å®¢æˆ·ç«¯ID
            search_id (str): æœç´¢ID
        Returns:
            Dict[str, Any]: æœç´¢çŠ¶æ€ä¿¡æ¯
        """
        task_id = f"{client_id}_{search_id}"
        
        with self._task_lock:
            # æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡
            if task_id in self.running_tasks:
                task_info = self.running_tasks[task_id]
                search_task = task_info["task"]
                return {
                    **search_task.search_data,
                    "task_status": "running",
                    "start_time": task_info["start_time"].isoformat()
                }
            
            # æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
            if task_id in self.completed_tasks:
                task_info = self.completed_tasks[task_id]
                return {
                    **task_info["result"],
                    "task_status": "completed",
                    "start_time": task_info["start_time"].isoformat(),
                    "completed_time": task_info["completed_time"].isoformat()
                }
        
        return {
            "status": "not_found", 
            "error": "æœç´¢è®°å½•ä¸å­˜åœ¨",
            "task_status": "not_found"
        }
    
    def get_all_tasks_status(self) -> Dict[str, Any]:
        """
        ğŸ“‹ è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€
        
        Returns:
            Dict[str, Any]: æ‰€æœ‰ä»»åŠ¡çŠ¶æ€
        """
        with self._task_lock:
            return {
                "running_tasks": len(self.running_tasks),
                "completed_tasks": len(self.completed_tasks),
                "max_workers": self.max_workers,
                "max_completed_tasks": self.max_completed_tasks,
                "running_task_ids": list(self.running_tasks.keys()),
                "completed_task_ids": self.completed_tasks_order.copy(),  # ğŸ“‹ æŒ‰æ—¶é—´é¡ºåºè¿”å›
                "completed_task_ids_count": len(self.completed_tasks_order)
            }
    
    def cancel_task(self, client_id: str, search_id: str) -> bool:
        """
        ğŸ›‘ å–æ¶ˆæœç´¢ä»»åŠ¡
        
        Args:
            client_id (str): å®¢æˆ·ç«¯ID
            search_id (str): æœç´¢ID
        Returns:
            bool: æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        task_id = f"{client_id}_{search_id}"
        
        with self._task_lock:
            if task_id in self.running_tasks:
                task_info = self.running_tasks[task_id]
                future = task_info["future"]
                
                # å°è¯•å–æ¶ˆä»»åŠ¡
                cancelled = future.cancel()
                
                if cancelled:
                    # æ¸…ç†èµ„æº
                    search_task = task_info["task"]
                    search_task._cleanup()
                    
                    self.running_tasks.pop(task_id)
                    logger.info(f"ğŸ›‘ ä»»åŠ¡ {task_id} å·²å–æ¶ˆ")
                    return True
                else:
                    logger.warning(f"âš ï¸ æ— æ³•å–æ¶ˆä»»åŠ¡ {task_id}ï¼Œä»»åŠ¡å¯èƒ½å·²åœ¨æ‰§è¡Œä¸­")
                    return False
        
        return False
    
    def get_recent_completed_tasks(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        ğŸ“‹ è·å–æœ€è¿‘å®Œæˆçš„ä»»åŠ¡åˆ—è¡¨
        
        Args:
            limit (int): è¿”å›çš„ä»»åŠ¡æ•°é‡é™åˆ¶
        Returns:
            List[Dict[str, Any]]: æœ€è¿‘å®Œæˆçš„ä»»åŠ¡åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
        """
        with self._task_lock:
            # è·å–æœ€è¿‘çš„ä»»åŠ¡IDï¼ˆå€’åºï¼‰
            recent_task_ids = self.completed_tasks_order[-limit:] if limit > 0 else self.completed_tasks_order
            recent_task_ids.reverse()  # æœ€æ–°çš„åœ¨å‰é¢
            
            recent_tasks = []
            for task_id in recent_task_ids:
                if task_id in self.completed_tasks:
                    task_data = self.completed_tasks[task_id].copy()
                    task_data["task_id"] = task_id
                    recent_tasks.append(task_data)
            
            return recent_tasks
    
    def shutdown(self, wait: bool = True):
        """
        ğŸ”’ å…³é—­çˆ¬è™«ç®¡ç†å™¨
        
        Args:
            wait (bool): æ˜¯å¦ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        """
        logger.info("ğŸ”’ æ­£åœ¨å…³é—­Google Scholarçˆ¬è™«ç®¡ç†å™¨...")
        
        with self._task_lock:
            # æ¸…ç†æ‰€æœ‰è¿è¡Œä¸­ä»»åŠ¡çš„èµ„æº
            for task_id, task_info in self.running_tasks.items():
                try:
                    search_task = task_info["task"]
                    search_task._cleanup()
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†ä»»åŠ¡ {task_id} èµ„æºå¤±è´¥: {str(e)}")
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=wait)
        logger.info("ğŸ”’ Google Scholarçˆ¬è™«ç®¡ç†å™¨å·²å…³é—­")


# åˆ›å»ºå…¨å±€å®ä¾‹
scholar_crawler = GoogleScholarCrawler(max_workers=5, headless=False) 