from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
import time
import re
from datetime import datetime
import logging
import threading
import concurrent.futures
import asyncio
from typing import List, Dict, Any, Optional
from schemas import WS_EVENTS, URLItem, PaperBase, URLItemStatus
from utils.socket_manager import socket_manager

logger = logging.getLogger(__name__)

class GoogleScholarSearchTask:
    """
    ğŸ“‹ å•ä¸ªGoogle Scholaræœç´¢ä»»åŠ¡
    æ¯ä¸ªä»»åŠ¡æ‹¥æœ‰ç‹¬ç«‹çš„æµè§ˆå™¨å®ä¾‹ï¼Œæ”¯æŒå¹¶å‘æ‰§è¡Œ
    """
    
    def __init__(self, google_scholar_url: str, client_id: str, search_id: str, headless: bool = False):
        """
        åˆå§‹åŒ–æœç´¢ä»»åŠ¡
        
        Args:
            google_scholar_url (str): Google Scholarä½œè€…é¡µé¢URL
            client_id (str): å®¢æˆ·ç«¯ID
            search_id (str): æœç´¢ID
            headless (bool): æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼
        """
        self.google_scholar_url = google_scholar_url
        self.client_id = client_id
        self.search_id = search_id
        self.headless = headless
        
        # ğŸ—ï¸ æµè§ˆå™¨é…ç½®
        self.options = Options()
        if headless:
            self.options.add_argument("--headless")
        self.options.add_argument("--disable-gpu")
        self.options.add_argument("--window-size=1200,800")
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        self.options.add_argument("--disable-blink-features=AutomationControlled")
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = None
        self.wait = None
        
        # ğŸ“Š æœç´¢çŠ¶æ€
        self.search_data = {
            "client_id": client_id,
            "search_id": search_id,
            "url": google_scholar_url,
            "author_name": "",
            "status": URLItemStatus.PENDING,
            "progress": 0,
            "fetched_paper_count": None,
            "total_paper_count": None,
            "papers_urls": [],
            "papers": [],
            "error_message": "",
            "start_time": datetime.now().isoformat(),
            "thread_id": threading.current_thread().ident
        }
        
        self.search_data = URLItem(**self.search_data)
    
    def _initialize_driver(self):
        """ğŸš€ åˆå§‹åŒ–ç‹¬ç«‹çš„WebDriverå®ä¾‹"""
        try:
            self.driver = webdriver.Chrome(options=self.options)
            self.wait = WebDriverWait(self.driver, 10)
            logger.info(f"ğŸš€ çº¿ç¨‹ {threading.current_thread().ident} WebDriverå·²åˆå§‹åŒ–")
            return True
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ {threading.current_thread().ident} WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = f"WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}"
            return False
    
    async def run(self) -> Dict[str, Any]:
        """
        ğŸ¯ æ‰§è¡Œå®Œæ•´çš„æœç´¢ä»»åŠ¡
        
        Returns:
            Dict[str, Any]: æœç´¢ç»“æœ
        """
        try:
            logger.info(f"ğŸ¯ çº¿ç¨‹ {threading.current_thread().ident} å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡: {self.search_id}")
            
            # ğŸš€ åˆå§‹åŒ–æµè§ˆå™¨
            if not self._initialize_driver():
                await socket_manager.send(WS_EVENTS["FAILED_FETCH_A_GOOGLE_SCHOLAR_URL"], self._serialize_search_data(), self.client_id)
                return self.search_data
            
            # ğŸ“‹ æ”¶é›†Scholarä¿¡æ¯
            if await self._collect_scholar_info():
                await socket_manager.send(WS_EVENTS["UPDATE_FETCH_A_GOOGLE_SCHOLAR_URL_PROCESS"], self._serialize_search_data(), self.client_id)
                # ğŸ“š æœç´¢è®ºæ–‡è¯¦ç»†ä¿¡æ¯
                await self._search_papers_details()
                await socket_manager.send(WS_EVENTS["FETCHED_COMPLETED_WITH_PAPERS_INFO"], self._serialize_search_data(), self.client_id)
            else:
                await socket_manager.send(WS_EVENTS["FAILED_FETCH_A_GOOGLE_SCHOLAR_URL"], self._serialize_search_data(), self.client_id)
            
            logger.info(f"âœ… çº¿ç¨‹ {threading.current_thread().ident} æœç´¢ä»»åŠ¡å®Œæˆ: {self.search_id}")
            
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ {threading.current_thread().ident} æœç´¢ä»»åŠ¡å¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = str(e)
        
        finally:
            # ğŸ”’ æ¸…ç†èµ„æº
            self._cleanup()
        
        return self.search_data
    
    def run_sync(self) -> Dict[str, Any]:
        """
        ğŸ”„ åŒæ­¥åŒ…è£…å™¨ï¼Œç”¨äºåœ¨ThreadPoolExecutorä¸­è¿è¡Œasyncæ–¹æ³•
        
        Returns:
            Dict[str, Any]: æœç´¢ç»“æœ
        """
        try:
            # ğŸ”„ åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œasyncæ–¹æ³•
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self.run())
            finally:
                loop.close()
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ {threading.current_thread().ident} åŒæ­¥åŒ…è£…å™¨æ‰§è¡Œå¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = str(e)
            return self.search_data
    
    async def _collect_scholar_info(self) -> bool:
        """
        ğŸ“‹ æ”¶é›†Google Scholarä½œè€…ä¿¡æ¯å’Œè®ºæ–‡åˆ—è¡¨
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.search_data.status = URLItemStatus.COLLECTING_INFO
            logger.info(f"ğŸ“‹ çº¿ç¨‹ {threading.current_thread().ident} å¼€å§‹æ”¶é›†Scholarä¿¡æ¯...")
            
            # ğŸŒ è®¿é—®URL
            logger.info(f"ğŸŒ æ­£åœ¨è®¿é—®: {self.google_scholar_url}")
            self.driver.get(self.google_scholar_url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # ğŸ‘¤ è·å–ä½œè€…ä¿¡æ¯
            if not await self._extract_author_info():
                return False
            
            # ğŸ“… æŒ‰å¹´ä»½æ’åºè®ºæ–‡
            self._sort_papers_by_year()
            
            # ğŸ“œ åŠ è½½æ‰€æœ‰è®ºæ–‡
            self._load_all_papers()
            
            # ğŸ“Š æ”¶é›†è®ºæ–‡URLs
            if not self._collect_paper_urls():
                return False
            
            self.search_data.fetched_paper_count = 0
            self.search_data.status = URLItemStatus.COLLECTED_INFO
            self.search_data.progress = 25
            logger.info(f"âœ… çº¿ç¨‹ {threading.current_thread().ident} Scholarä¿¡æ¯æ”¶é›†å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹ {threading.current_thread().ident} æ”¶é›†Scholarä¿¡æ¯å¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = f"æ”¶é›†Scholarä¿¡æ¯å¤±è´¥: {str(e)}"
            return False
    
    async def _extract_author_info(self) -> bool:
        """ğŸ‘¤ æå–ä½œè€…ä¿¡æ¯"""
        try:
            author_selectors = [
                "#gsc_prf_in",
                ".gsc_prf_in", 
                "h1",
                ".gs_ai_name"
            ]
            
            author_name = ""
            for selector in author_selectors:
                try:
                    author_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    author_name = author_element.text.strip()
                    if author_name:
                        logger.info(f"âœ… ä¿¡æ¯åŒºåŸŸ: {selector}")
                        break
                except NoSuchElementException:
                    continue
            
            if not author_name:
                page_title = self.driver.title
                if " - " in page_title:
                    logger.info(f"âœ… é¡µé¢æ ‡é¢˜: {page_title} åˆ†å‰²å: {page_title.split(' - ')[0].strip()}")
                    author_name = page_title.split(" - ")[0].strip()
            
            if author_name:
                self.search_data.author_name = author_name
                logger.info(f"âœ… ä½œè€…ä¿¡æ¯: {author_name}")
                await socket_manager.send(WS_EVENTS["UPDATE_FETCH_A_GOOGLE_SCHOLAR_URL_PROCESS"], self._serialize_search_data(), self.client_id)
                return True
            else:
                logger.error("âŒ æ— æ³•è·å–ä½œè€…ä¿¡æ¯")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æå–ä½œè€…ä¿¡æ¯å¤±è´¥: {str(e)}")
            return False
    
    def _sort_papers_by_year(self):
        """ğŸ“… æŒ‰å¹´ä»½æ’åºè®ºæ–‡"""
        try:
            logger.info("ğŸ“… æ­£åœ¨æŒ‰å¹´ä»½æ’åºè®ºæ–‡...")
            
            sort_selectors = [
                "#gsc_a_ha",
                "button[aria-label*='Sort']",
                ".gsc_a_ha"
            ]
            
            sort_button = None
            for selector in sort_selectors:
                try:
                    sort_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if sort_button and sort_button.is_displayed():
                        logger.info(f"âœ… æ’åºæŒ‰é’®: {selector}")
                        break
                except NoSuchElementException:
                    continue
            
            if sort_button:
                self.driver.execute_script("arguments[0].click();", sort_button)
                time.sleep(1)
                
                year_sort_options = [
                    "//button[contains(text(), 'Year')]",
                    "//a[contains(text(), 'Year')]",
                    "//option[contains(text(), 'Year')]"
                ]
                
                for xpath in year_sort_options:
                    try:
                        year_option = self.driver.find_element(By.XPATH, xpath)
                        logger.info(f"âœ… å¹´ä»½æ’åºé€‰é¡¹: {xpath}")
                        self.driver.execute_script("arguments[0].click();", year_option)
                        time.sleep(1)
                        break
                    except NoSuchElementException:
                        continue
            
            logger.info("âœ… è®ºæ–‡å·²æŒ‰å¹´ä»½æ’åº")
            
        except Exception as e:
            logger.warning(f"âš ï¸ æŒ‰å¹´ä»½æ’åºå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ’åº: {str(e)}")
    
    def _load_all_papers(self):
        """ğŸ“œ åŠ è½½æ‰€æœ‰è®ºæ–‡ï¼ˆç‚¹å‡»Show moreï¼‰"""
        try:
            logger.info("ğŸ“œ æ­£åœ¨åŠ è½½æ‰€æœ‰è®ºæ–‡...")
            show_more_attempts = 0
            max_attempts = 1000
            
            initial_papers = self.driver.find_elements(By.CSS_SELECTOR, ".gsc_a_tr")
            logger.info(f"ğŸ“‹ åˆå§‹è®ºæ–‡æ•°é‡: {len(initial_papers)}")
            
            while show_more_attempts < max_attempts:
                try:
                    show_more_selectors = [
                        "#gsc_bpf_more",
                        "button[onclick*='more']",
                        ".gsc_bpf_more",
                        "//button[contains(text(), 'Show more')]",
                        "//button[contains(text(), 'SHOW MORE')]",
                        "//a[contains(text(), 'Show more')]"
                    ]
                    
                    show_more_button = None
                    for selector in show_more_selectors:
                        try:
                            if selector.startswith("//"):
                                show_more_button = self.driver.find_element(By.XPATH, selector)
                            else:
                                show_more_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                            
                            if show_more_button and show_more_button.is_displayed() and show_more_button.is_enabled():
                                break
                        except NoSuchElementException:
                            continue
                    
                    if show_more_button and show_more_button.is_displayed() and show_more_button.is_enabled():
                        self.driver.execute_script("arguments[0].scrollIntoView(true);", show_more_button)
                        time.sleep(1)
                        self.driver.execute_script("arguments[0].click();", show_more_button)
                        logger.info(f"ğŸ”„ ç‚¹å‡»Show moreæŒ‰é’® (ç¬¬{show_more_attempts + 1}æ¬¡)")
                        time.sleep(2)
                        show_more_attempts += 1
                    else:
                        logger.info("âœ… æ²¡æœ‰æ›´å¤šè®ºæ–‡å¯åŠ è½½")
                        break
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ç‚¹å‡»Show moreæŒ‰é’®å¤±è´¥: {str(e)}")
                    break
            
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½è®ºæ–‡å¤±è´¥: {str(e)}")
    
    def _collect_paper_urls(self) -> bool:
        """ğŸ“Š æ”¶é›†æ‰€æœ‰è®ºæ–‡URL"""
        try:
            logger.info("ğŸ“Š æ­£åœ¨æ”¶é›†è®ºæ–‡ä¿¡æ¯...")
            
            paper_selectors = [
                ".gsc_a_tr",
                "tr.gsc_a_tr",
                ".gs_r.gs_or.gs_scl",
                ".gsc_a_t"
            ]
            
            papers_urls = []
            paper_elements = []
            
            for selector in paper_selectors:
                try:
                    paper_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if paper_elements:
                        break
                except NoSuchElementException:
                    continue
            
            if paper_elements:
                for paper_element in paper_elements:
                    try:
                        link_selectors = [
                            "a.gsc_a_at",
                            "a",
                            ".gsc_a_at"
                        ]
                        
                        paper_url = None
                        for link_selector in link_selectors:
                            try:
                                link_element = paper_element.find_element(By.CSS_SELECTOR, link_selector)
                                paper_url = link_element.get_attribute("href")
                                if paper_url and paper_url.startswith("http"):
                                    break
                            except NoSuchElementException:
                                continue
                        
                        if paper_url and paper_url not in papers_urls:
                            papers_urls.append(paper_url)
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ æå–è®ºæ–‡URLå¤±è´¥: {str(e)}")
                        continue
            
            total_paper_count = len(papers_urls)
            self.search_data.total_paper_count = total_paper_count
            self.search_data.papers_urls = papers_urls
            
            logger.info(f"âœ… æˆåŠŸæ”¶é›†åˆ° {total_paper_count} ç¯‡è®ºæ–‡")
            logger.info(f"ğŸ“‹ è®ºæ–‡URLæ ·ä¾‹: {papers_urls[:3] if papers_urls else 'æ— '}")
            
            return total_paper_count > 0
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†è®ºæ–‡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return False
    
    async def _search_papers_details(self):
        """ğŸ“š æœç´¢è®ºæ–‡è¯¦ç»†ä¿¡æ¯"""
        try:
            papers_urls = self.search_data.papers_urls
            if not papers_urls:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡URL")
                return
            
            logger.info(f"ğŸ“š å¼€å§‹è·å– {len(papers_urls)} ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯...")
            self.search_data.status = URLItemStatus.SEARCHING_PAPERS
            
            papers = []
            
            for index, paper_url in enumerate(papers_urls):
                self.search_data.fetched_paper_count = index + 1
                self.search_data.progress = 25 + round((index + 1)*1.0 / len(papers_urls) * 70, 2)
                try:
                    logger.info(f"ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {index + 1}/{len(papers_urls)} ç¯‡è®ºæ–‡...")
                    
                    # ğŸŒ è®¿é—®è®ºæ–‡é¡µé¢
                    self.driver.get(paper_url)
                    time.sleep(2)
                    
                    # ğŸ“‹ æå–è®ºæ–‡ä¿¡æ¯
                    paper_info = self._extract_paper_details(paper_url)
                    
                    if paper_info:
                        papers.append(paper_info)
                        logger.info(f"âœ… æˆåŠŸæå–è®ºæ–‡: {paper_info.title[:50]}...")
                    else:
                        logger.warning(f"âš ï¸ æ— æ³•æå–è®ºæ–‡ä¿¡æ¯: {paper_url}")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†è®ºæ–‡å¤±è´¥ {paper_url}: {str(e)}")
                    continue
                await socket_manager.send(WS_EVENTS["UPDATE_FETCH_A_GOOGLE_SCHOLAR_URL_PROCESS"], self._serialize_search_data(), self.client_id)
                logger.info(f"âœ… æˆåŠŸæå–è®ºæ–‡:\n{paper_info}")
            # ğŸ“Š æ›´æ–°æœ€ç»ˆç»“æœ
            self.search_data.papers = papers
            self.search_data.status = URLItemStatus.COMPLETED
            self.search_data.progress = 100
            
            logger.info(f"âœ… è®ºæ–‡è¯¦ç»†ä¿¡æ¯æœç´¢å®Œæˆï¼ŒæˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢è®ºæ–‡è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
            self.search_data.status = URLItemStatus.ERROR
            self.search_data.error_message = f"æœç´¢è®ºæ–‡è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}"
    
    def _serialize_search_data(self) -> dict:
        """
        ğŸ”„ åºåˆ—åŒ–æœç´¢æ•°æ®ï¼Œå¤„ç†datetimeç­‰ä¸å¯JSONåºåˆ—åŒ–çš„å¯¹è±¡
        
        Returns:
            dict: å¯JSONåºåˆ—åŒ–çš„æœç´¢æ•°æ®
        """
        try:
            # ä½¿ç”¨model_dumpå¹¶æŒ‡å®šåºåˆ—åŒ–æ¨¡å¼
            return self.search_data.model_dump(mode='json')
        except Exception as e:
            logger.warning(f"âš ï¸ åºåˆ—åŒ–æœç´¢æ•°æ®å¤±è´¥: {str(e)}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨å¤„ç†
            data = self.search_data.model_dump()
            
            # å¤„ç†datetimeå­—æ®µ
            if 'start_time' in data and data['start_time']:
                if isinstance(data['start_time'], str):
                    pass  # å·²ç»æ˜¯å­—ç¬¦ä¸²
                else:
                    data['start_time'] = data['start_time'].isoformat() if data['start_time'] else None
            
            # å¤„ç†papersä¸­çš„datetimeå­—æ®µ
            if 'papers' in data and data['papers']:
                for paper in data['papers']:
                    if 'date' in paper and paper['date']:
                        if not isinstance(paper['date'], str):
                            paper['date'] = paper['date'].isoformat() if paper['date'] else ""
            
            return data
    
    def _extract_paper_details(self, paper_url: str) -> Optional[PaperBase]:
        """ğŸ“„ æå–å•ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ - åŸºäºGoogle Scholarè¯¦æƒ…é¡µé¢ç»“æ„"""
        try:
            paper_info = {
                "title": "",
                "authors": [],
                "year": 0,
                "date": "",
                "url": paper_url,
                "pdf_url": None,
                "citations": 0,
                "publisher": None,
                "paper_type": None,
                "description": None
            }
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#gsc_oci_table, #gsc_oci_title, h1"))
                )
            except TimeoutException:
                logger.warning("âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–ä¿¡æ¯")
            
            # ğŸ“‘ æå–æ ‡é¢˜ - ä»gsc_oci_titleæˆ–é¡µé¢æ ‡é¢˜
            title_selectors = [
                "#gsc_oci_title .gsc_oci_title_link",
                "#gsc_oci_title a",
                "#gsc_oci_title",
                "h1",
                "title"
            ]
            
            for selector in title_selectors:
                try:
                    title_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    title = title_element.text.strip()
                    if title and len(title) > 5 and title != "View article":
                        paper_info["title"] = title
                        logger.debug(f"ğŸ“‘ æ‰¾åˆ°æ ‡é¢˜: {title}")
                        break
                except NoSuchElementException:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä»metaæ ‡ç­¾è·å–
            if not paper_info["title"]:
                try:
                    meta_title = self.driver.find_element(By.CSS_SELECTOR, "meta[property='og:title']")
                    title = meta_title.get_attribute("content")
                    if title and len(title) > 5:
                        paper_info["title"] = title.strip()
                        logger.debug(f"ğŸ“‘ ä»metaæ ‡ç­¾è·å–æ ‡é¢˜: {title}")
                except NoSuchElementException:
                    pass
            
            # ğŸ“Š æå–è®ºæ–‡è¯¦æƒ…è¡¨æ ¼ä¿¡æ¯
            try:
                # è·å–æ‰€æœ‰å­—æ®µ-å€¼å¯¹
                field_elements = self.driver.find_elements(By.CSS_SELECTOR, "#gsc_oci_table .gs_scl")
                logger.debug(f"ğŸ“Š æ‰¾åˆ° {len(field_elements)} ä¸ªå­—æ®µ")
                
                for field_element in field_elements:
                    try:
                        field_name_element = field_element.find_element(By.CSS_SELECTOR, ".gsc_oci_field")
                        field_value_element = field_element.find_element(By.CSS_SELECTOR, ".gsc_oci_value")
                        
                        field_name = field_name_element.text.strip().lower()
                        field_value = field_value_element.text.strip()
                        
                        if not field_value:
                            continue
                        
                        logger.debug(f"ğŸ“Š å¤„ç†å­—æ®µ: {field_name} = {field_value[:50]}...")
                        
                        # ğŸ‘¥ å¤„ç†ä½œè€…ä¿¡æ¯
                        if field_name == "authors":
                            authors = self._parse_authors(field_value)
                            if authors:
                                paper_info["authors"] = authors
                                logger.debug(f"ğŸ‘¥ è§£æä½œè€…: {authors}")
                        
                        # ğŸ“… å¤„ç†å‘è¡¨æ—¥æœŸ
                        elif field_name == "publication date":
                            year, date = self._parse_date_info(field_value)
                            if year and year > 0:
                                paper_info["year"] = year
                                paper_info["date"] = date
                                logger.debug(f"ğŸ“… è§£ææ—¥æœŸ: {year}, {date}")
                        
                        # ğŸ“° å¤„ç†å‘è¡¨å•†ä¿¡æ¯ï¼ˆJournal/Book/Conferenceï¼‰
                        elif field_name in ["book", "journal", "conference", "venue"]:
                            paper_info["publisher"] = field_value
                            # æ ¹æ®å­—æ®µç±»å‹æ¨æ–­è®ºæ–‡ç±»å‹
                            if field_name == "journal":
                                paper_info["paper_type"] = "Journal"
                            elif field_name in ["book", "conference"]:
                                paper_info["paper_type"] = "Conference"
                            else:
                                paper_info["paper_type"] = self._infer_paper_type(field_value)
                            logger.debug(f"ğŸ“° è§£æå‘è¡¨å•†: {field_value}, ç±»å‹: {paper_info['paper_type']}")
                        
                        # ğŸ“ å¤„ç†æè¿°ä¿¡æ¯
                        elif field_name == "description":
                            # è·å–æ›´è¯¦ç»†çš„æè¿°å†…å®¹
                            try:
                                desc_element = field_element.find_element(By.CSS_SELECTOR, ".gsh_csp")
                                description = desc_element.text.strip()
                                if description and len(description) > 20:
                                    paper_info["description"] = description  # å¢åŠ æè¿°é•¿åº¦é™åˆ¶
                                    logger.debug(f"ğŸ“ è§£ææè¿°: {description[:100]}...")
                            except NoSuchElementException:
                                if len(field_value) > 20:
                                    paper_info["description"] = field_value
                                    logger.debug(f"ğŸ“ ä½¿ç”¨å­—æ®µå€¼ä½œä¸ºæè¿°: {field_value[:100]}...")
                        
                        # ğŸ“Š å¤„ç†å¼•ç”¨æ¬¡æ•°
                        elif field_name == "total citations":
                            citations = self._parse_citations(field_value)
                            if citations is not None:
                                paper_info["citations"] = citations
                                logger.debug(f"ğŸ“Š è§£æå¼•ç”¨æ¬¡æ•°: {citations}")
                        
                    except NoSuchElementException:
                        continue
                        
            except NoSuchElementException:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°è®ºæ–‡è¯¦æƒ…è¡¨æ ¼")
            
            # ğŸ”— æå–PDFé“¾æ¥ - ä»gsc_oci_title_ggåŒºåŸŸ
            try:
                pdf_link_element = self.driver.find_element(By.CSS_SELECTOR, "#gsc_oci_title_gg a")
                pdf_url = pdf_link_element.get_attribute("href")
                if pdf_url and (".pdf" in pdf_url or "arxiv.org" in pdf_url):
                    paper_info["pdf_url"] = pdf_url
                    logger.debug(f"ğŸ”— æ‰¾åˆ°PDFé“¾æ¥: {pdf_url}")
            except NoSuchElementException:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾å…¶ä»–PDFé“¾æ¥
                pdf_selectors = [
                    "a[href*='.pdf']",
                    "a[href*='arxiv.org/pdf']",
                    "a[href*='doi.org']"
                ]
                
                for selector in pdf_selectors:
                    try:
                        pdf_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        for pdf_element in pdf_elements:
                            href = pdf_element.get_attribute("href")
                            if href and (href.endswith('.pdf') or 'arxiv.org/pdf' in href):
                                paper_info["pdf_url"] = href
                                logger.debug(f"ğŸ”— å¤‡ç”¨æ–¹æ¡ˆæ‰¾åˆ°PDFé“¾æ¥: {href}")
                                break
                        if paper_info["pdf_url"]:
                            break
                    except NoSuchElementException:
                        continue
            
            # âœ… éªŒè¯å¿…éœ€å­—æ®µ
            if not paper_info["title"]:
                logger.error("âŒ æœªæ‰¾åˆ°è®ºæ–‡æ ‡é¢˜")
                return None
            
            # ğŸ”§ è®¾ç½®é»˜è®¤å€¼
            if not paper_info["date"]:
                paper_info["date"] = f"{paper_info['year']}-01-01" if paper_info["year"] > 0 else "1900-01-01"
            
            if not paper_info["paper_type"]:
                paper_info["paper_type"] = "Unknown"
            
            # ğŸ”§ åˆ›å»ºPaperBaseå¯¹è±¡
            try:
                paper_base = PaperBase(**paper_info)
                logger.info(f"âœ… æˆåŠŸæå–è®ºæ–‡ä¿¡æ¯: {paper_info['title'][:50]}...")
                return paper_base
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºPaperBaseå¯¹è±¡å¤±è´¥: {str(e)}")
                logger.debug(f"ğŸ“Š è®ºæ–‡ä¿¡æ¯: {paper_info}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ æå–è®ºæ–‡è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def _parse_authors(self, authors_text: str) -> List[str]:
        """ğŸ‘¥ è§£æä½œè€…ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # ç§»é™¤å¸¸è§çš„éä½œè€…ä¿¡æ¯
            authors_text = re.sub(r'\s*-\s*.*$', '', authors_text)  # ç§»é™¤ " - " åçš„å†…å®¹
            authors_text = re.sub(r'\s*\d{4}.*$', '', authors_text)  # ç§»é™¤å¹´ä»½åçš„å†…å®¹
            
            # åˆ†å‰²ä½œè€…
            authors = []
            if ',' in authors_text:
                authors = [author.strip() for author in authors_text.split(',')]
            else:
                # å¤„ç†æ²¡æœ‰é€—å·åˆ†éš”çš„æƒ…å†µ
                authors = [authors_text.strip()]
            
            # æ¸…ç†ä½œè€…åç§°
            cleaned_authors = []
            for author in authors:
                # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
                author = re.sub(r'[^\w\s\-\.\']', '', author).strip()
                author = re.sub(r'\d+', '', author).strip()
                
                # éªŒè¯ä½œè€…åç§°
                if author and len(author) > 1 and not author.isdigit():
                    cleaned_authors.append(author)
            
            return cleaned_authors[:10]  # é™åˆ¶ä½œè€…æ•°é‡
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§£æä½œè€…ä¿¡æ¯å¤±è´¥: {str(e)}")
            return []
    
    def _parse_date_info(self, date_text: str) -> tuple:
        """ğŸ“… è§£ææ—¥æœŸä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # å°è¯•è§£æä¸åŒçš„æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2023/10/21
                r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2023-10-21
                r'(\d{1,2})/(\d{1,2})/(\d{4})',  # 10/21/2023
                r'(\d{1,2})-(\d{1,2})-(\d{4})',  # 10-21-2023
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, date_text)
                if match:
                    groups = match.groups()
                    if len(groups) == 3:
                        if len(groups[0]) == 4:  # YYYY/MM/DD or YYYY-MM-DD
                            year, month, day = int(groups[0]), int(groups[1]), int(groups[2])
                        else:  # MM/DD/YYYY or MM-DD-YYYY
                            month, day, year = int(groups[0]), int(groups[1]), int(groups[2])
                        
                        # éªŒè¯æ—¥æœŸåˆç†æ€§
                        if 1900 <= year <= 2030 and 1 <= month <= 12 and 1 <= day <= 31:
                            date_str = f"{year}-{month:02d}-{day:02d}"
                            return year, date_str
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´æ—¥æœŸï¼Œå°è¯•åªæå–å¹´ä»½
            year_match = re.search(r'\b(19|20)\d{2}\b', date_text)
            if year_match:
                year = int(year_match.group())
                if 1900 <= year <= 2030:
                    return year, f"{year}-01-01"
            
            return 0, ""
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§£ææ—¥æœŸä¿¡æ¯å¤±è´¥: {str(e)}")
            return 0, ""
    
    def _parse_citations(self, citation_text: str) -> int:
        """ğŸ“Š è§£æå¼•ç”¨æ¬¡æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # æŸ¥æ‰¾ "Cited by X" æ¨¡å¼
            cited_by_match = re.search(r'Cited by (\d+)', citation_text, re.IGNORECASE)
            if cited_by_match:
                return int(cited_by_match.group(1))
            
            # æŸ¥æ‰¾çº¯æ•°å­—
            number_match = re.search(r'\b(\d+)\b', citation_text)
            if number_match:
                return int(number_match.group(1))
            
            return 0
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§£æå¼•ç”¨æ¬¡æ•°å¤±è´¥: {str(e)}")
            return 0
    
    def _infer_paper_type(self, publisher_text: str) -> str:
        """ğŸ“° æ¨æ–­è®ºæ–‡ç±»å‹"""
        try:
            text_lower = publisher_text.lower()
            
            # æœŸåˆŠå…³é”®è¯
            journal_keywords = ['journal', 'nature', 'science', 'ieee', 'acm transactions', 'plos']
            if any(keyword in text_lower for keyword in journal_keywords):
                return "Journal"
            
            # ä¼šè®®å…³é”®è¯
            conference_keywords = ['conference', 'proceedings', 'workshop', 'symposium', 'acm', 'ieee']
            if any(keyword in text_lower for keyword in conference_keywords):
                return "Conference"
            
            # é¢„å°æœ¬å…³é”®è¯
            preprint_keywords = ['arxiv', 'preprint', 'biorxiv', 'medrxiv']
            if any(keyword in text_lower for keyword in preprint_keywords):
                return "Preprint"
            
            return "Unknown"
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨æ–­è®ºæ–‡ç±»å‹å¤±è´¥: {str(e)}")
            return "Unknown"
    
    def test_paper_detail_extraction(self, paper_url: str) -> Optional[PaperBase]:
        """ğŸ§ª æµ‹è¯•è®ºæ–‡è¯¦æƒ…æå–åŠŸèƒ½"""
        try:
            logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•è®ºæ–‡è¯¦æƒ…æå–: {paper_url}")
            
            # è®¿é—®è®ºæ–‡è¯¦æƒ…é¡µé¢
            self.driver.get(paper_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "body"))
            )
            
            # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
            time.sleep(3)
            
            # æå–è®ºæ–‡è¯¦æƒ…
            paper_details = self._extract_paper_details(paper_url)
            
            if paper_details:
                logger.info(f"âœ… æµ‹è¯•æˆåŠŸï¼æå–åˆ°è®ºæ–‡ä¿¡æ¯:")
                logger.info(f"ğŸ“‘ æ ‡é¢˜: {paper_details.title}")
                logger.info(f"ğŸ‘¥ ä½œè€…: {paper_details.authors}")
                logger.info(f"ğŸ“… å¹´ä»½: {paper_details.year}")
                logger.info(f"ğŸ“Š å¼•ç”¨: {paper_details.citations}")
                logger.info(f"ğŸ“° å‘è¡¨å•†: {paper_details.publisher}")
                logger.info(f"ğŸ“‹ ç±»å‹: {paper_details.paper_type}")
                logger.info(f"ğŸ”— PDF: {paper_details.pdf_url}")
                if paper_details.description:
                    logger.info(f"ğŸ“ æè¿°: {paper_details.description[:100]}...")
            else:
                logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼æœªèƒ½æå–è®ºæ–‡ä¿¡æ¯")
            
            return paper_details
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def _cleanup(self):
        """ğŸ”’ æ¸…ç†èµ„æº"""
        try:
            if self.driver:
                self.driver.quit()
                self.driver = None
                self.wait = None
                logger.info(f"ğŸ”’ çº¿ç¨‹ {threading.current_thread().ident} æµè§ˆå™¨å·²å…³é—­")
        except Exception as e:
            logger.warning(f"âš ï¸ çº¿ç¨‹ {threading.current_thread().ident} å…³é—­æµè§ˆå™¨å¤±è´¥: {str(e)}")


class GoogleScholarCrawler:
    """
    ğŸ” Google Scholar è®ºæ–‡çˆ¬è™«ç®¡ç†å™¨
    æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘æœç´¢ï¼Œæ¯ä¸ªä»»åŠ¡ä½¿ç”¨ç‹¬ç«‹çš„æµè§ˆå™¨å®ä¾‹
    """
    
    def __init__(self, max_workers: int = 3, headless: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«ç®¡ç†å™¨
        
        Args:
            max_workers (int): æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            headless (bool): æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼
        """
        self.max_workers = max_workers
        self.headless = headless
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        self.running_tasks = {}  # å­˜å‚¨æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        self.completed_tasks = {}  # å­˜å‚¨å·²å®Œæˆçš„ä»»åŠ¡
        self.completed_tasks_order = []  # ğŸ“‹ è®°å½•å®Œæˆä»»åŠ¡çš„é¡ºåº
        self.max_completed_tasks = 20  # ğŸ”¢ æœ€å¤§ä¿å­˜çš„å®Œæˆä»»åŠ¡æ•°é‡
        self._task_lock = threading.Lock()
        
        logger.info(f"ğŸš€ Google Scholar çˆ¬è™«ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
    
    def scholar_info(self, google_scholar_url: str, client_id: str, search_id: str) -> str:
        """
        ğŸ¯ æäº¤æœç´¢ä»»åŠ¡ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
        
        Args:
            google_scholar_url (str): Google Scholarä½œè€…é¡µé¢URL
            client_id (str): å®¢æˆ·ç«¯ID
            search_id (str): æœç´¢ID
        Returns:
            str: ä»»åŠ¡ID
        """
        task_id = f"{client_id}_{search_id}"
        
        with self._task_lock:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒä»»åŠ¡åœ¨è¿è¡Œ
            if task_id in self.running_tasks:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} å·²åœ¨è¿è¡Œä¸­")
                return task_id
            
            # åˆ›å»ºæœç´¢ä»»åŠ¡
            search_task = GoogleScholarSearchTask(
                google_scholar_url=google_scholar_url,
                client_id=client_id,
                search_id=search_id,
                headless=self.headless
            )
            
            # æäº¤åˆ°çº¿ç¨‹æ± ï¼ˆä½¿ç”¨åŒæ­¥åŒ…è£…å™¨ï¼‰
            future = self.executor.submit(search_task.run_sync)
            
            # æ·»åŠ å›è°ƒå‡½æ•°å¤„ç†å®Œæˆçš„ä»»åŠ¡
            future.add_done_callback(lambda f: self._on_task_complete(task_id, f))
            
            self.running_tasks[task_id] = {
                "future": future,
                "task": search_task,
                "start_time": datetime.now()
            }
            
            logger.info(f"ğŸ¯ ä»»åŠ¡ {task_id} å·²æäº¤åˆ°çº¿ç¨‹æ± ")
            return task_id
    
    def _on_task_complete(self, task_id: str, future: concurrent.futures.Future):
        """ğŸ“Š ä»»åŠ¡å®Œæˆå›è°ƒ"""
        with self._task_lock:
            if task_id in self.running_tasks:
                task_info = self.running_tasks.pop(task_id)
                
                try:
                    result = future.result()
                    self.completed_tasks[task_id] = {
                        "result": result,
                        "completed_time": datetime.now(),
                        "start_time": task_info["start_time"]
                    }
                    logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²å®Œæˆ")
                except Exception as e:
                    logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {str(e)}")
                    self.completed_tasks[task_id] = {
                        "result": {"status": "error", "error_message": str(e)},
                        "completed_time": datetime.now(),
                        "start_time": task_info["start_time"]
                    }
                
                # ğŸ“‹ ç®¡ç†å®Œæˆä»»åŠ¡åˆ—è¡¨ï¼Œä¿æŒæœ€å¤š20ä¸ªè®°å½•
                self._manage_completed_tasks(task_id)
    
    def _manage_completed_tasks(self, new_task_id: str):
        """
        ğŸ“‹ ç®¡ç†å®Œæˆä»»åŠ¡åˆ—è¡¨ï¼Œä¿æŒæœ€å¤š20ä¸ªè®°å½•ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
        
        Args:
            new_task_id (str): æ–°å®Œæˆçš„ä»»åŠ¡ID
        """
        # æ·»åŠ æ–°ä»»åŠ¡åˆ°é¡ºåºåˆ—è¡¨
        if new_task_id not in self.completed_tasks_order:
            self.completed_tasks_order.append(new_task_id)
        
        # ğŸ”¢ å¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼Œåˆ é™¤æœ€æ—©çš„ä»»åŠ¡
        while len(self.completed_tasks_order) > self.max_completed_tasks:
            oldest_task_id = self.completed_tasks_order.pop(0)  # ç§»é™¤æœ€æ—©çš„ä»»åŠ¡ID
            
            if oldest_task_id in self.completed_tasks:
                del self.completed_tasks[oldest_task_id]  # åˆ é™¤å¯¹åº”çš„ä»»åŠ¡æ•°æ®
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤æœ€æ—©çš„å®Œæˆä»»åŠ¡è®°å½•: {oldest_task_id}")
        
        logger.info(f"ğŸ“Š å½“å‰ä¿å­˜ {len(self.completed_tasks_order)} ä¸ªå®Œæˆä»»åŠ¡è®°å½•")
    
    def get_search_status(self, client_id: str, search_id: str) -> Dict[str, Any]:
        """
        ğŸ“Š è·å–æœç´¢çŠ¶æ€ä¿¡æ¯
        
        Args:
            client_id (str): å®¢æˆ·ç«¯ID
            search_id (str): æœç´¢ID
        Returns:
            Dict[str, Any]: æœç´¢çŠ¶æ€ä¿¡æ¯
        """
        task_id = f"{client_id}_{search_id}"
        
        with self._task_lock:
            # æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡
            if task_id in self.running_tasks:
                task_info = self.running_tasks[task_id]
                search_task = task_info["task"]
                return {
                    **search_task.search_data,
                    "task_status": "running",
                    "start_time": task_info["start_time"].isoformat()
                }
            
            # æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
            if task_id in self.completed_tasks:
                task_info = self.completed_tasks[task_id]
                return {
                    **task_info["result"],
                    "task_status": "completed",
                    "start_time": task_info["start_time"].isoformat(),
                    "completed_time": task_info["completed_time"].isoformat()
                }
        
        return {
            "status": "not_found", 
            "error": "æœç´¢è®°å½•ä¸å­˜åœ¨",
            "task_status": "not_found"
        }
    
    def get_all_tasks_status(self) -> Dict[str, Any]:
        """
        ğŸ“‹ è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€
        
        Returns:
            Dict[str, Any]: æ‰€æœ‰ä»»åŠ¡çŠ¶æ€
        """
        with self._task_lock:
            return {
                "running_tasks": len(self.running_tasks),
                "completed_tasks": len(self.completed_tasks),
                "max_workers": self.max_workers,
                "max_completed_tasks": self.max_completed_tasks,
                "running_task_ids": list(self.running_tasks.keys()),
                "completed_task_ids": self.completed_tasks_order.copy(),  # ğŸ“‹ æŒ‰æ—¶é—´é¡ºåºè¿”å›
                "completed_task_ids_count": len(self.completed_tasks_order)
            }
    
    def cancel_task(self, client_id: str, search_id: str) -> bool:
        """
        ğŸ›‘ å–æ¶ˆæœç´¢ä»»åŠ¡
        
        Args:
            client_id (str): å®¢æˆ·ç«¯ID
            search_id (str): æœç´¢ID
        Returns:
            bool: æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        task_id = f"{client_id}_{search_id}"
        
        with self._task_lock:
            if task_id in self.running_tasks:
                task_info = self.running_tasks[task_id]
                future = task_info["future"]
                
                # å°è¯•å–æ¶ˆä»»åŠ¡
                cancelled = future.cancel()
                
                if cancelled:
                    # æ¸…ç†èµ„æº
                    search_task = task_info["task"]
                    search_task._cleanup()
                    
                    self.running_tasks.pop(task_id)
                    logger.info(f"ğŸ›‘ ä»»åŠ¡ {task_id} å·²å–æ¶ˆ")
                    return True
                else:
                    logger.warning(f"âš ï¸ æ— æ³•å–æ¶ˆä»»åŠ¡ {task_id}ï¼Œä»»åŠ¡å¯èƒ½å·²åœ¨æ‰§è¡Œä¸­")
                    return False
        
        return False
    
    def get_recent_completed_tasks(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        ğŸ“‹ è·å–æœ€è¿‘å®Œæˆçš„ä»»åŠ¡åˆ—è¡¨
        
        Args:
            limit (int): è¿”å›çš„ä»»åŠ¡æ•°é‡é™åˆ¶
        Returns:
            List[Dict[str, Any]]: æœ€è¿‘å®Œæˆçš„ä»»åŠ¡åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
        """
        with self._task_lock:
            # è·å–æœ€è¿‘çš„ä»»åŠ¡IDï¼ˆå€’åºï¼‰
            recent_task_ids = self.completed_tasks_order[-limit:] if limit > 0 else self.completed_tasks_order
            recent_task_ids.reverse()  # æœ€æ–°çš„åœ¨å‰é¢
            
            recent_tasks = []
            for task_id in recent_task_ids:
                if task_id in self.completed_tasks:
                    task_data = self.completed_tasks[task_id].copy()
                    task_data["task_id"] = task_id
                    recent_tasks.append(task_data)
            
            return recent_tasks
    
    def shutdown(self, wait: bool = True):
        """
        ğŸ”’ å…³é—­çˆ¬è™«ç®¡ç†å™¨
        
        Args:
            wait (bool): æ˜¯å¦ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        """
        logger.info("ğŸ”’ æ­£åœ¨å…³é—­Google Scholarçˆ¬è™«ç®¡ç†å™¨...")
        
        with self._task_lock:
            # æ¸…ç†æ‰€æœ‰è¿è¡Œä¸­ä»»åŠ¡çš„èµ„æº
            for task_id, task_info in self.running_tasks.items():
                try:
                    search_task = task_info["task"]
                    search_task._cleanup()
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†ä»»åŠ¡ {task_id} èµ„æºå¤±è´¥: {str(e)}")
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=wait)
        logger.info("ğŸ”’ Google Scholarçˆ¬è™«ç®¡ç†å™¨å·²å…³é—­")


# åˆ›å»ºå…¨å±€å®ä¾‹
scholar_crawler = GoogleScholarCrawler(max_workers=5, headless=False) 